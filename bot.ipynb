{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTnLjDvKnVVqQfVA7QGhXXdV8Eo7fOFc_dYp4HMfqSzcTdYuSJI4GccuxeaBgzGtH7m1M9GfLZkzFST/pub?gid=2008373479&single=true&output=csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('interior_design_qa.csv')\n",
    "# df = pd.read_csv('intents_qa.csv')\n",
    "df=pd.read_csv('qa_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi there</td>\n",
       "      <td>Hello! Am Bot Here to help you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How are you</td>\n",
       "      <td>Good to see you again! Am Bot Here to help you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>Hi there, how can I help?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey</td>\n",
       "      <td>Hello! Am Bot Here to help you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hola</td>\n",
       "      <td>Hello! Am Bot Here to help you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Are you expensive?</td>\n",
       "      <td>I'm a free virtual assistant provided for info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Who’s your boss / master?</td>\n",
       "      <td>I don't have a boss or master. I operate based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Do you get smarter?</td>\n",
       "      <td>I don't inherently get smarter on my own. My p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Address</td>\n",
       "      <td>Delhi Laxmi Interior is located near [Include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>where to visit you</td>\n",
       "      <td>Delhi Laxmi Interior is located near [Include ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Question  \\\n",
       "0                    Hi there   \n",
       "1                 How are you   \n",
       "2            Is anyone there?   \n",
       "3                         Hey   \n",
       "4                        Hola   \n",
       "..                        ...   \n",
       "74         Are you expensive?   \n",
       "75  Who’s your boss / master?   \n",
       "76        Do you get smarter?   \n",
       "77                   Address    \n",
       "78         where to visit you   \n",
       "\n",
       "                                               Answer  \n",
       "0                     Hello! Am Bot Here to help you.  \n",
       "1     Good to see you again! Am Bot Here to help you.  \n",
       "2                           Hi there, how can I help?  \n",
       "3                     Hello! Am Bot Here to help you.  \n",
       "4                     Hello! Am Bot Here to help you.  \n",
       "..                                                ...  \n",
       "74  I'm a free virtual assistant provided for info...  \n",
       "75  I don't have a boss or master. I operate based...  \n",
       "76  I don't inherently get smarter on my own. My p...  \n",
       "77  Delhi Laxmi Interior is located near [Include ...  \n",
       "78  Delhi Laxmi Interior is located near [Include ...  \n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = df['Question'].tolist()\n",
    "answers_list = df['Answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyaspeller in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (2.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyaspeller) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->pyaspeller) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->pyaspeller) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->pyaspeller) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->pyaspeller) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaspeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmer =  PorterStemmer() \n",
    "    text = re.sub(r'[^\\w\\s]','',text)  # removing non alphanumeric characters \n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    lemmatized_tokens  = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens  = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stopwords(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "# X = vectorizer.fit_transform([preprocess(q) for q in questions_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------Not using -------------------------\n",
    "\n",
    "# # Function to calculate chatbot response\n",
    "# def chatbot_response(input_text):\n",
    "#     processed_input = preprocess_with_stopwords(input_text)\n",
    "\n",
    "#     # Use the same vectorizer instance for both X and vectorized_input\n",
    "#     vectorized_input = vectorizer.transform([processed_input])\n",
    "\n",
    "#     similarities = cosine_similarity(vectorized_input, X)\n",
    "#     max_similarity = np.max(similarities)\n",
    "\n",
    "#     if max_similarity > 0.6:\n",
    "#         high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "#         target_answers = [answers_list[questions_list.index(q)] for q in high_similarity_questions]\n",
    "\n",
    "#         # Use the same vectorizer instance for both Z and vectorized_input_with_stopwords\n",
    "#         Z = vectorizer.transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "#         vectorized_input_with_stopwords = vectorizer.transform([processed_input])\n",
    "\n",
    "#         final_similarities = cosine_similarity(vectorized_input_with_stopwords, Z)\n",
    "#         closest = np.argmax(final_similarities)\n",
    "\n",
    "#         return target_answers[closest]\n",
    "#     else:\n",
    "#         return \"I'm sorry, I don't understand. Could you please rephrase or provide more details?\"\n",
    "\n",
    "# # Sample usage\n",
    "# user_input = \"lvng room design ideas\"\n",
    "# response = chatbot_response(user_input)\n",
    "# print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We offer a range of services, including residential and commercial interior design, space planning, color consultation, and furniture selection. Our goal is to create spaces that reflect your style and meet your functional needs.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "def chatbot_response(input_text):\n",
    "    processed_input = preprocess(input_text)\n",
    "    vectorized_input = vectorizer.transform([processed_input])\n",
    "\n",
    "    similarities = cosine_similarity(vectorized_input, X)\n",
    "    max_similarity = np.max(similarities)\n",
    "\n",
    "    # Dynamic threshold based on the 75th percentile\n",
    "    threshold = np.percentile(similarities, 75)\n",
    "\n",
    "    if max_similarity > threshold:\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > threshold]\n",
    "        target_answers = [answers_list[questions_list.index(q)] for q in high_similarity_questions]\n",
    "\n",
    "        Z = vectorizer.transform([preprocess(q) for q in high_similarity_questions])\n",
    "        final_similarities = cosine_similarity(vectorized_input, Z)\n",
    "        closest = np.argmax(final_similarities)\n",
    "\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"To know more call/whatsapp us on +918779693725 or email us at ankitmishra.letter@gmail.com\"\n",
    "\n",
    "# Sample usage\n",
    "user_input = \"what are your services\"\n",
    "response = chatbot_response(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hey\n",
      "Chatbot: Hello! Am Bot Here to help you.\n",
      "You: hey\n",
      "Chatbot: Hello! Am Bot Here to help you.\n",
      "You: what are your services\n",
      "Chatbot: We offer a range of services, including residential and commercial interior design, space planning, color consultation, and furniture selection. Our goal is to create spaces that reflect your style and meet your functional needs.\n",
      "You: \n",
      "Chatbot: To know more call/whatsapp us on +918779693725 or email us at ankitmishra.letter@gmail.com\n",
      "You: address ?\n",
      "Chatbot: Delhi Laxmi Interior is located near [Include Specific Location Details]. For more precise directions, you may want to use a map or contact them directly.\n",
      "You: \n",
      "Chatbot: To know more call/whatsapp us on +918779693725 or email us at ankitmishra.letter@gmail.com\n",
      "You: give contact info\n",
      "Chatbot: You can reach us at [Your Contact Number]. Feel free to call us or send an email to [Your Email Address] for inquiries or to schedule a consultation.\n",
      "You: \n",
      "Chatbot: To know more call/whatsapp us on +918779693725 or email us at ankitmishra.letter@gmail.com\n",
      "You: \n",
      "Chatbot: To know more call/whatsapp us on +918779693725 or email us at ankitmishra.letter@gmail.com\n",
      "Exiting the chat.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # Exit the loop if the user types \"exit\" or \"quit\"\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the chat.\")\n",
    "        break\n",
    "    \n",
    "    print(\"You:\", user_input)\n",
    "    \n",
    "    response = chatbot_response(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  what is data analytics ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyaspeller import YandexSpeller\n",
    "\n",
    "def error_correct_pyspeller(sample_text):\n",
    "    speller = YandexSpeller()\n",
    "    fixed = speller.spelled(sample_text)\n",
    "    return fixed\n",
    "\n",
    "input_text = \"\"\"\n",
    " what is data anlytics ?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_text = error_correct_pyspeller(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's up?\n"
     ]
    }
   ],
   "source": [
    "import Caribe as cb\n",
    "\n",
    "\n",
    "sentence = \"wha is hi nae\"\n",
    "standard = cb.caribe_corrector(sentence)\n",
    "\n",
    "print(standard) # Outputs: I am playing football outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sample paragraph with some incorrect spelling and grammar mistakes.\n",
      "It's important to check large text chunks for accuracy and improve readability.\n",
      "Ginger it is a great library for such tasks, and it can handle large text as well.\n",
      "\n",
      "Let's try processing this large text using Ginger it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyaspeller import YandexSpeller\n",
    "import language_tool_python\n",
    "\n",
    "def error_correcting(text):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    datasets = tool.correct(text)\n",
    "    return datasets\n",
    "\n",
    "def error_correct_pyspeller(sample_text):\n",
    "    speller = YandexSpeller()\n",
    "    fixed = speller.spelled(sample_text)\n",
    "    return fixed\n",
    "\n",
    "input_text = \"\"\"\n",
    "This is a sample paragrap with some incorrect spellings and grammer mistaks.\n",
    "It's importnt to check larje text chunks for accurcy and improve readibility.\n",
    "Gingerit is a great library for such tasks, and it can handl larje text as well.\n",
    "\n",
    "Let's try processing this larje text using Gingerit.\n",
    "\"\"\"\n",
    "\n",
    "# output_data = error_correcting(input_text)\n",
    "# print(output_data)\n",
    "\n",
    "output_text = error_correct_pyspeller(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gingerit.gingerit import GingerIt\n",
    "\n",
    "# text = 'What is Data Anlytics'\n",
    "\n",
    "# parser = GingerIt()\n",
    "# corrected_text = parser.parse(text)\n",
    "\n",
    "# print(corrected_text['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st Concept \n",
    "\n",
    "#####  Some question who has no match will be saved in other file, \n",
    "##### Training Data is not available \n",
    "\n",
    "\n",
    "2nd Concept \n",
    "\n",
    "\n",
    "#####  List of Qs & Answers \n",
    "using LLM & keep storing those qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
